version: '3.8'

x-logging: &default-logging
  driver: json-file
  options:
    max-size: '10m'
    max-file: '5'

x-healthcheck: &basic-healthcheck
  interval: 30s
  timeout: 5s
  retries: 3
  start_period: 30s

x-rebalance-env: &rebalance-env
  CSE_REBALANCE_ENABLED: "1"
  CSE_REBALANCE_INTERVAL: "300"
  CSE_REBALANCE_METHOD: "rp"
  CSE_REBALANCE_USE_REAL_COV: "1"
  CSE_REBALANCE_VOL_TARGET_ENABLED: "1"
  CSE_REBALANCE_VOL_TARGET: "0.01"
  CSE_REBALANCE_DRY_RUN: "0"
  CSE_REBALANCE_MAX_ORDERS: "10"
  CSE_REBALANCE_FEE_RATE: "0.001"
  CSE_REBALANCE_SLIPPAGE_BPS: "5"
  CSE_REBALANCE_MIN_NOTIONAL: "10.0"
  CSE_REBALANCE_BACKOFF_ENABLED: "1"
  CSE_REBALANCE_BACKOFF_MULT: "2.0"
  CSE_REBALANCE_BACKOFF_MAX: "3600"
  CSE_REBALANCE_PROMETHEUS: "1"
  CSE_REBALANCE_PROM_PORT: "9001"

networks:
  frontend:
    driver: overlay
    attachable: true
  backend:
    driver: overlay
    attachable: true
  monitoring:
    driver: overlay
    attachable: true

secrets:
  api_keys_encrypted:
    external: true
  arbitrage_env:
    external: true
  rebalance_env:
    external: true

configs:
  nginx_conf:
    external: true
  prometheus_conf:
    external: true
  grafana_dashboards:
    external: true

services:
  # Service principal avec rebalance intégré
  cryptospreadedge-main:
    image: cryptospreadedge:latest
    command: ["python", "scripts/trading/start_trading_system.py", "--mode", "live"]
    deploy:
      replicas: 2
      mode: replicated
      endpoint_mode: vip
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.2
      rollback_config:
        parallelism: 1
        delay: 5s
        monitor: 60s
        order: stop-first
      resources:
        reservations:
          cpus: '1.5'
          memory: 3G
        limits:
          cpus: '3.0'
          memory: 8G
      placement:
        constraints:
          - node.role == worker
          - node.labels.tier == trading
    environment:
      - REDIS_URL=redis://redis:6379
      - INFLUXDB_URL=http://influxdb:8086
      - KAFKA_BROKERS=kafka:9092
      - CSE_MODE=live
      - CSE_SYMBOLS=BTC,ETH,BNB,ADA,DOT,SOL,MATIC,AVAX
      - CSE_UPDATE_INTERVAL=60
      <<: *rebalance-env
    secrets:
      - source: api_keys_encrypted
        target: /run/secrets/api_keys.encrypted
      - source: arbitrage_env
        target: /run/secrets/arbitrage.env
      - source: rebalance_env
        target: /run/secrets/rebalance.env
    configs:
      - source: prometheus_conf
        target: /etc/prometheus/prometheus.yml
        mode: 0444
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:9001/metrics', timeout=5)"]
      <<: *basic-healthcheck
    volumes:
      - type: bind
        source: /var/lib/cryptospreadedge/data
        target: /app/data
      - type: bind
        source: /var/lib/cryptospreadedge/logs
        target: /app/logs
      - type: bind
        source: /var/lib/cryptospreadedge/portfolio
        target: /app/portfolio
    logging: *default-logging
    networks:
      - backend
      - monitoring

  # Service dédié pour arbitrage intensif
  arbitrage-worker:
    image: cryptospreadedge:latest
    command: ["python", "-m", "src.scripts.arbitrage.start_arbitrage"]
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
      placement:
        constraints:
          - node.role == worker
          - node.labels.tier == arbitrage
    environment:
      - REDIS_URL=redis://redis:6379
      - INFLUXDB_URL=http://influxdb:8086
      - KAFKA_BROKERS=kafka:9092
    secrets:
      - source: api_keys_encrypted
        target: /run/secrets/api_keys.encrypted
      - source: arbitrage_env
        target: /run/secrets/arbitrage.env
    healthcheck:
      test: ["CMD", "python", "-c", "import os,sys; sys.exit(0)"]
      <<: *basic-healthcheck
    logging: *default-logging
    networks:
      - backend

  # Service dédié pour le rebalance de portefeuille (scalable indépendamment)
  portfolio-rebalancer:
    image: cryptospreadedge:latest
    command: ["python", "-m", "src.scripts.tools.portfolio_cli", "rebalance", "--method", "rp", "--dry-run", "0"]
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '1.5'
          memory: 3G
      placement:
        constraints:
          - node.role == worker
          - node.labels.tier == portfolio
    environment:
      - REDIS_URL=redis://redis:6379
      - INFLUXDB_URL=http://influxdb:8086
      - KAFKA_BROKERS=kafka:9092
      <<: *rebalance-env
    secrets:
      - source: api_keys_encrypted
        target: /run/secrets/api_keys.encrypted
      - source: rebalance_env
        target: /run/secrets/rebalance.env
    healthcheck:
      test: ["CMD", "python", "-c", "import os,sys; sys.exit(0)"]
      <<: *basic-healthcheck
    volumes:
      - type: bind
        source: /var/lib/cryptospreadedge/portfolio
        target: /app/portfolio
    logging: *default-logging
    networks:
      - backend
      - monitoring

  # Service de monitoring des métriques de rebalance
  portfolio-monitor:
    image: cryptospreadedge:latest
    command: ["python", "-c", "import time; import requests; from http.server import HTTPServer, BaseHTTPRequestHandler; class Handler(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200); self.end_headers(); self.wfile.write(b'Portfolio Monitor OK'); HTTPServer(('0.0.0.0', 9002), Handler).serve_forever()"]
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.1'
          memory: 128M
        limits:
          cpus: '0.25'
          memory: 256M
      placement:
        constraints:
          - node.role == manager
    ports:
      - "9002:9002"
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:9002"]
      <<: *basic-healthcheck
    logging: *default-logging
    networks:
      - monitoring

  # API Gateway / Nginx avec load balancing optimisé
  nginx:
    image: nginx:alpine
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.1'
          memory: 128M
        limits:
          cpus: '0.5'
          memory: 256M
      placement:
        constraints:
          - node.role == manager
    ports:
      - "80:80"
      - "443:443"
      - "9001:9001"  # Port pour métriques rebalance
      - "9002:9002"  # Port pour monitoring portfolio
    configs:
      - source: nginx_conf
        target: /etc/nginx/nginx.conf
        mode: 0444
    depends_on:
      - cryptospreadedge-main
    logging: *default-logging
    networks:
      - frontend
      - backend
      - monitoring

  # Redis avec persistence optimisée
  redis:
    image: redis:7-alpine
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '1.0'
          memory: 2G
      placement:
        constraints:
          - node.role == manager
    volumes:
      - type: volume
        source: redis_data
        target: /data
    command: ["redis-server", "--appendonly", "yes", "--maxmemory", "1gb", "--maxmemory-policy", "allkeys-lru"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      <<: *basic-healthcheck
    logging: *default-logging
    networks:
      - backend

  # InfluxDB avec rétention optimisée pour données de portefeuille
  influxdb:
    image: influxdb:2.7-alpine
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
      placement:
        constraints:
          - node.role == manager
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=password123
      - DOCKER_INFLUXDB_INIT_ORG=cryptospreadedge
      - DOCKER_INFLUXDB_INIT_BUCKET=trading_data
      - DOCKER_INFLUXDB_INIT_RETENTION=30d
    volumes:
      - type: volume
        source: influxdb_data
        target: /var/lib/influxdb2
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:8086/health"]
      <<: *basic-healthcheck
    logging: *default-logging
    networks:
      - backend

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '0.5'
          memory: 1G
      placement:
        constraints:
          - node.role == manager
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    logging: *default-logging
    networks:
      - backend

  # Kafka avec topics pour portefeuille
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
      placement:
        constraints:
          - node.role == manager
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    volumes:
      - type: volume
        source: kafka_data
        target: /var/lib/kafka/data
    logging: *default-logging
    networks:
      - backend

  # Prometheus avec configuration pour métriques de rebalance
  prometheus:
    image: prom/prometheus:latest
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '1.0'
          memory: 2G
      placement:
        constraints:
          - node.role == manager
    volumes:
      - type: volume
        source: prometheus_data
        target: /prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    configs:
      - source: prometheus_conf
        target: /etc/prometheus/prometheus.yml
        mode: 0444
    logging: *default-logging
    networks:
      - monitoring

  # Grafana avec dashboards pour portefeuille
  grafana:
    image: grafana/grafana:latest
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '1.0'
          memory: 2G
      placement:
        constraints:
          - node.role == manager
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - type: volume
        source: grafana_data
        target: /var/lib/grafana
    configs:
      - source: grafana_dashboards
        target: /etc/grafana/provisioning/dashboards
        mode: 0444
    logging: *default-logging
    networks:
      - frontend
      - monitoring

  # Elasticsearch pour logs centralisés
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.5'
          memory: 1G
        limits:
          cpus: '2.0'
          memory: 4G
      placement:
        constraints:
          - node.role == manager
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    volumes:
      - type: volume
        source: elasticsearch_data
        target: /usr/share/elasticsearch/data
    logging: *default-logging
    networks:
      - backend

  # Kibana pour visualisation des logs
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
      resources:
        reservations:
          cpus: '0.25'
          memory: 512M
        limits:
          cpus: '1.0'
          memory: 2G
      placement:
        constraints:
          - node.role == manager
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    logging: *default-logging
    networks:
      - frontend
      - backend

volumes:
  redis_data: {}
  influxdb_data: {}
  kafka_data: {}
  prometheus_data: {}
  grafana_data: {}
  elasticsearch_data: {}